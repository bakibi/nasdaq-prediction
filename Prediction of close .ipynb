{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pandas version:       0.22.0\n",
      "NumPy version:        1.14.2\n",
      "SciKit Learn version: 0.19.1\n",
      "TensorFlow version:   1.6.0\n",
      "MatPlotLib version:   2.2.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import sklearn\n",
    "from sklearn.model_selection import KFold\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from dateutil.parser import parse\n",
    "from datetime import datetime, timedelta\n",
    "from collections import deque\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "print(\"Pandas version:      \", pd.__version__)\n",
    "print(\"NumPy version:       \", np.__version__)\n",
    "print(\"SciKit Learn version:\", sklearn.__version__)\n",
    "print(\"TensorFlow version:  \", tf.__version__)\n",
    "print(\"MatPlotLib version:  \", matplotlib.__version__)\n",
    "\n",
    "seed = 8\n",
    "tf.set_random_seed(seed)\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>open</th>\n",
       "      <th>close</th>\n",
       "      <th>low</th>\n",
       "      <th>high</th>\n",
       "      <th>volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>851264.000000</td>\n",
       "      <td>851264.000000</td>\n",
       "      <td>851264.000000</td>\n",
       "      <td>851264.000000</td>\n",
       "      <td>8.512640e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>70.836986</td>\n",
       "      <td>70.857109</td>\n",
       "      <td>70.118414</td>\n",
       "      <td>71.543476</td>\n",
       "      <td>5.415113e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>83.695876</td>\n",
       "      <td>83.689686</td>\n",
       "      <td>82.877294</td>\n",
       "      <td>84.465504</td>\n",
       "      <td>1.249468e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.850000</td>\n",
       "      <td>0.860000</td>\n",
       "      <td>0.830000</td>\n",
       "      <td>0.880000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>33.840000</td>\n",
       "      <td>33.849998</td>\n",
       "      <td>33.480000</td>\n",
       "      <td>34.189999</td>\n",
       "      <td>1.221500e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>52.770000</td>\n",
       "      <td>52.799999</td>\n",
       "      <td>52.230000</td>\n",
       "      <td>53.310001</td>\n",
       "      <td>2.476250e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>79.879997</td>\n",
       "      <td>79.889999</td>\n",
       "      <td>79.110001</td>\n",
       "      <td>80.610001</td>\n",
       "      <td>5.222500e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1584.439941</td>\n",
       "      <td>1578.130005</td>\n",
       "      <td>1549.939941</td>\n",
       "      <td>1600.930054</td>\n",
       "      <td>8.596434e+08</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                open          close            low           high  \\\n",
       "count  851264.000000  851264.000000  851264.000000  851264.000000   \n",
       "mean       70.836986      70.857109      70.118414      71.543476   \n",
       "std        83.695876      83.689686      82.877294      84.465504   \n",
       "min         0.850000       0.860000       0.830000       0.880000   \n",
       "25%        33.840000      33.849998      33.480000      34.189999   \n",
       "50%        52.770000      52.799999      52.230000      53.310001   \n",
       "75%        79.879997      79.889999      79.110001      80.610001   \n",
       "max      1584.439941    1578.130005    1549.939941    1600.930054   \n",
       "\n",
       "             volume  \n",
       "count  8.512640e+05  \n",
       "mean   5.415113e+06  \n",
       "std    1.249468e+07  \n",
       "min    0.000000e+00  \n",
       "25%    1.221500e+06  \n",
       "50%    2.476250e+06  \n",
       "75%    5.222500e+06  \n",
       "max    8.596434e+08  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe = pd.read_csv('prices.csv')\n",
    "dataframe.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>symbol</th>\n",
       "      <th>open</th>\n",
       "      <th>close</th>\n",
       "      <th>low</th>\n",
       "      <th>high</th>\n",
       "      <th>volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>851254</th>\n",
       "      <td>2016-12-30</td>\n",
       "      <td>XRAY</td>\n",
       "      <td>58.290001</td>\n",
       "      <td>57.730000</td>\n",
       "      <td>57.540001</td>\n",
       "      <td>58.360001</td>\n",
       "      <td>949200.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>851255</th>\n",
       "      <td>2016-12-30</td>\n",
       "      <td>XRX</td>\n",
       "      <td>8.720000</td>\n",
       "      <td>8.730000</td>\n",
       "      <td>8.700000</td>\n",
       "      <td>8.800000</td>\n",
       "      <td>11250400.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>851256</th>\n",
       "      <td>2016-12-30</td>\n",
       "      <td>XYL</td>\n",
       "      <td>49.980000</td>\n",
       "      <td>49.520000</td>\n",
       "      <td>49.360001</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>646200.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>851257</th>\n",
       "      <td>2016-12-30</td>\n",
       "      <td>YHOO</td>\n",
       "      <td>38.720001</td>\n",
       "      <td>38.669998</td>\n",
       "      <td>38.430000</td>\n",
       "      <td>39.000000</td>\n",
       "      <td>6431600.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>851258</th>\n",
       "      <td>2016-12-30</td>\n",
       "      <td>YUM</td>\n",
       "      <td>63.930000</td>\n",
       "      <td>63.330002</td>\n",
       "      <td>63.160000</td>\n",
       "      <td>63.939999</td>\n",
       "      <td>1887100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>851259</th>\n",
       "      <td>2016-12-30</td>\n",
       "      <td>ZBH</td>\n",
       "      <td>103.309998</td>\n",
       "      <td>103.199997</td>\n",
       "      <td>102.849998</td>\n",
       "      <td>103.930000</td>\n",
       "      <td>973800.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>851260</th>\n",
       "      <td>2016-12-30</td>\n",
       "      <td>ZION</td>\n",
       "      <td>43.070000</td>\n",
       "      <td>43.040001</td>\n",
       "      <td>42.689999</td>\n",
       "      <td>43.310001</td>\n",
       "      <td>1938100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>851261</th>\n",
       "      <td>2016-12-30</td>\n",
       "      <td>ZTS</td>\n",
       "      <td>53.639999</td>\n",
       "      <td>53.529999</td>\n",
       "      <td>53.270000</td>\n",
       "      <td>53.740002</td>\n",
       "      <td>1701200.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>851262</th>\n",
       "      <td>2016-12-30 00:00:00</td>\n",
       "      <td>AIV</td>\n",
       "      <td>44.730000</td>\n",
       "      <td>45.450001</td>\n",
       "      <td>44.410000</td>\n",
       "      <td>45.590000</td>\n",
       "      <td>1380900.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>851263</th>\n",
       "      <td>2016-12-30 00:00:00</td>\n",
       "      <td>FTV</td>\n",
       "      <td>54.200001</td>\n",
       "      <td>53.630001</td>\n",
       "      <td>53.389999</td>\n",
       "      <td>54.480000</td>\n",
       "      <td>705100.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       date symbol        open       close         low  \\\n",
       "851254           2016-12-30   XRAY   58.290001   57.730000   57.540001   \n",
       "851255           2016-12-30    XRX    8.720000    8.730000    8.700000   \n",
       "851256           2016-12-30    XYL   49.980000   49.520000   49.360001   \n",
       "851257           2016-12-30   YHOO   38.720001   38.669998   38.430000   \n",
       "851258           2016-12-30    YUM   63.930000   63.330002   63.160000   \n",
       "851259           2016-12-30    ZBH  103.309998  103.199997  102.849998   \n",
       "851260           2016-12-30   ZION   43.070000   43.040001   42.689999   \n",
       "851261           2016-12-30    ZTS   53.639999   53.529999   53.270000   \n",
       "851262  2016-12-30 00:00:00    AIV   44.730000   45.450001   44.410000   \n",
       "851263  2016-12-30 00:00:00    FTV   54.200001   53.630001   53.389999   \n",
       "\n",
       "              high      volume  \n",
       "851254   58.360001    949200.0  \n",
       "851255    8.800000  11250400.0  \n",
       "851256   50.000000    646200.0  \n",
       "851257   39.000000   6431600.0  \n",
       "851258   63.939999   1887100.0  \n",
       "851259  103.930000    973800.0  \n",
       "851260   43.310001   1938100.0  \n",
       "851261   53.740002   1701200.0  \n",
       "851262   45.590000   1380900.0  \n",
       "851263   54.480000    705100.0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe.tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>open</th>\n",
       "      <th>close</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>123.430000</td>\n",
       "      <td>125.839996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>125.239998</td>\n",
       "      <td>119.980003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>116.379997</td>\n",
       "      <td>114.949997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>115.480003</td>\n",
       "      <td>116.620003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>117.010002</td>\n",
       "      <td>114.970001</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         open       close\n",
       "0  123.430000  125.839996\n",
       "1  125.239998  119.980003\n",
       "2  116.379997  114.949997\n",
       "3  115.480003  116.620003\n",
       "4  117.010002  114.970001"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "desired_columns = ['open', 'close']\n",
    "basic_mlp_data = dataframe[desired_columns]\n",
    "basic_mlp_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ffff\n"
     ]
    }
   ],
   "source": [
    "# Call this in IPython notebooks before any elements are added to\n",
    "# the default graph otherwise if you rerun cells you can get \n",
    "# annoying errors.\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Define the Neural Network topology with the 'net_hidden_sizes'\n",
    "# and how much we should regularise it and how quickly it should\n",
    "# learn. Also the type of non linearity we should use.\n",
    "amount_epochs = 50\n",
    "learning_rate = 0.001\n",
    "batch_size = 128\n",
    "net_hidden_sizes = [128, 64, 8]\n",
    "l2_strength = 0.01\n",
    "non_linearity = tf.nn.relu\n",
    "dropout_amount = 0.7\n",
    "\n",
    "# The input to the graph - the targets (close) and the inputs\n",
    "# (open). Also a placeholder to pass a variable dropout rate. \n",
    "net_input = tf.placeholder(tf.float32, shape=[None, 1])\n",
    "net_target = tf.placeholder(tf.float32, shape=[None, 1])\n",
    "dropout_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "# L2 regularisation to penalise the weights from growing too\n",
    "# large. Useful to prevent overfitting.\n",
    "regulariser = tf.contrib.layers.l2_regularizer(scale=l2_strength)\n",
    "\n",
    "# Build the network from the list of dimensions. Apply l2 and\n",
    "# dropout regularisation to the layers.\n",
    "net = net_input\n",
    "for size in net_hidden_sizes:\n",
    "    net = tf.layers.dense(inputs=net, \n",
    "                          units=size, \n",
    "                          activation=non_linearity, \n",
    "                          kernel_regularizer=regulariser)\n",
    "    net = tf.layers.dropout(inputs=net,\n",
    "                            rate=dropout_prob)\n",
    "\n",
    "# The models prediction has a linear output. \n",
    "net_output = tf.layers.dense(inputs=net,\n",
    "                             units=1, \n",
    "                             activation=None, \n",
    "                             kernel_regularizer=regulariser)    \n",
    "\n",
    "# The main loss for penalising the network on how well it does.\n",
    "loss = tf.losses.mean_squared_error(labels=net_target, \n",
    "                                    predictions=net_output)\n",
    "\n",
    "# TensorFlows manner of applying l2 to the loss.\n",
    "l2_variables = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\n",
    "l2_loss = tf.contrib.layers.apply_regularization(regulariser, \n",
    "                                                 l2_variables)\n",
    "total_loss = loss + l2_loss\n",
    "\n",
    "# Train and initialisation TensorFlow operations to be ran\n",
    "# in the session.\n",
    "train_op = tf.train.AdamOptimizer(learning_rate).minimize(total_loss)\n",
    "init_op = tf.global_variables_initializer()\n",
    "print('ffff')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fold iteration:   0 \n",
      "Error mean:       2.7146893 \n",
      "Error deviation:  3.3762183 \n",
      "\n",
      "\n",
      "Fold iteration:   1 \n",
      "Error mean:       3.507859 \n",
      "Error deviation:  3.1102047 \n",
      "\n",
      "\n",
      "Fold iteration:   2 \n",
      "Error mean:       2.104382 \n",
      "Error deviation:  2.184533 \n",
      "\n",
      "\n",
      "Fold iteration:   3 \n",
      "Error mean:       2.0846407 \n",
      "Error deviation:  1.8512129 \n",
      "\n",
      "\n",
      "Fold iteration:   4 \n",
      "Error mean:       1.8507316 \n",
      "Error deviation:  1.9735212 \n",
      "\n",
      "fff\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    \n",
    "    amount_folds = 5\n",
    "    k_folds = KFold(n_splits=amount_folds)\n",
    "    data = basic_mlp_data.as_matrix()\n",
    "    fold_errors = []\n",
    "    fold_iteration = 0\n",
    "    \n",
    "    # Cross validate the dataset, using K-Fold.\n",
    "    for train_indices, test_indices in k_folds.split(data):\n",
    "\n",
    "        # Each new fold, reinitialise the network.\n",
    "        sess.run(init_op)\n",
    "        \n",
    "        # Training phase.\n",
    "        for epoch in range(amount_epochs):\n",
    "            \n",
    "            # Each new epoch, reshuffle the train set.\n",
    "            random_train_indices = np.random.permutation(train_indices)\n",
    "            train_set = data[random_train_indices]\n",
    "            \n",
    "            # Loop over the train set and optimise the network.\n",
    "            for begin in range(0, len(train_set), batch_size):\n",
    "                end = begin + batch_size\n",
    "                batch_x = train_set[begin:end].T[0].reshape((-1, 1))\n",
    "                batch_y = train_set[begin:end].T[1].reshape((-1, 1))\n",
    "                \n",
    "                sess.run(train_op, feed_dict={\n",
    "                    net_input: batch_x,\n",
    "                    net_target: batch_y,\n",
    "                    dropout_prob: dropout_amount\n",
    "                })\n",
    "        \n",
    "        # Testing phase.\n",
    "        test_set = data[test_indices]\n",
    "        \n",
    "        # Collate the error over the test set.\n",
    "        all_error = []\n",
    "        for begin in range(0, len(test_set), batch_size):\n",
    "            end = begin + batch_size \n",
    "            batch_x = train_set[begin:end].T[0].reshape((-1, 1))\n",
    "            batch_y = train_set[begin:end].T[1].reshape((-1, 1))\n",
    "            \n",
    "            error = sess.run(loss, feed_dict={\n",
    "                net_input: batch_x,\n",
    "                net_target: batch_y,\n",
    "                dropout_prob: 1.0\n",
    "            }) \n",
    "            all_error.append(error)\n",
    "        \n",
    "        all_error = np.array(all_error).reshape((-1))\n",
    "        fold_errors.append(all_error)\n",
    "        \n",
    "        print(\"\\nFold iteration:  \", fold_iteration,\n",
    "              \"\\nError mean:      \", np.mean(all_error),\n",
    "              \"\\nError deviation: \", np.std(all_error),\n",
    "              \"\\n\")\n",
    "        fold_iteration += 1      \n",
    "        \n",
    "    fold_errors = np.array(fold_errors).reshape((amount_folds, -1))\n",
    "    print('fff')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dgdfgd\n"
     ]
    }
   ],
   "source": [
    "print('dgdfgd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 6 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "hist_data = dict()\n",
    "keys = ['fold 0', 'fold 1', 'fold 2', 'fold 3', 'fold 4']\n",
    "for i, key in enumerate(keys):\n",
    "    hist_data[key] = fold_errors[i]\n",
    "    \n",
    "\n",
    "plt.figure()\n",
    "hist_dataframe = pd.DataFrame(hist_data, columns=keys)\n",
    "_ = hist_dataframe.hist(color='k', alpha=0.5, bins=50, figsize=(10, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>symbol</th>\n",
       "      <th>close</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>251</th>\n",
       "      <td>2010-01-04</td>\n",
       "      <td>A</td>\n",
       "      <td>31.300001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>718</th>\n",
       "      <td>2010-01-05</td>\n",
       "      <td>A</td>\n",
       "      <td>30.960001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1186</th>\n",
       "      <td>2010-01-06</td>\n",
       "      <td>A</td>\n",
       "      <td>30.850001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1654</th>\n",
       "      <td>2010-01-07</td>\n",
       "      <td>A</td>\n",
       "      <td>30.809999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2122</th>\n",
       "      <td>2010-01-08</td>\n",
       "      <td>A</td>\n",
       "      <td>30.800000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            date symbol      close\n",
       "251   2010-01-04      A  31.300001\n",
       "718   2010-01-05      A  30.960001\n",
       "1186  2010-01-06      A  30.850001\n",
       "1654  2010-01-07      A  30.809999\n",
       "2122  2010-01-08      A  30.800000"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequence_desired_cols = ['date', 'symbol', 'close']\n",
    "sequence_data = dataframe[sequence_desired_cols]\n",
    "sequence_data = sequence_data.sort_values(by=['symbol', 'date'])\n",
    "sequence_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_length = 3\n",
    "\n",
    "all_data = deque()\n",
    "\n",
    "previous_date = None\n",
    "previous_class = None\n",
    "reset = False\n",
    "close_deque = deque()\n",
    "\n",
    "for index, row in sequence_data.iterrows():\n",
    "    current_date = parse(row['date'])\n",
    "    current_class = row['symbol']\n",
    "    \n",
    "    first_value = previous_date == None and previous_class == None\n",
    "    if not first_value:\n",
    "        successive_date = current_date == (previous_date + timedelta(days=1))\n",
    "    successive_class = current_class == previous_class\n",
    "    append = first_value or (successive_date and successive_class)\n",
    "\n",
    "    if append:\n",
    "        close_deque.append(row['close'])\n",
    "        \n",
    "        if len(close_deque) > sequence_length:\n",
    "            target_close = close_deque.pop()\n",
    "            all_data.append((list(close_deque), [target_close]))\n",
    "            reset = True\n",
    "            \n",
    "    if not append or reset:\n",
    "        close_deque.clear()\n",
    "        previous_date = None\n",
    "        previous_class = None\n",
    "        reset = False\n",
    "        \n",
    "    previous_date = current_date\n",
    "    previous_class = current_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x shape: (146379, 3, 1) \n",
      "y shape: (146379, 1)\n",
      "sequence difference mean: 1.0803335451465035 \n",
      "sequence difference max:  401.91332466666665 \n",
      "sequence difference min:  0.0 \n",
      "sequence difference std:  2.889675018801449\n"
     ]
    }
   ],
   "source": [
    "x_data, y_data = list(zip(*all_data))\n",
    "x_data = np.array(x_data).reshape((-1, sequence_length, 1))\n",
    "y_data = np.array(y_data)\n",
    "print(\"x shape:\", x_data.shape, \n",
    "      \"\\ny shape:\", y_data.shape)\n",
    "\n",
    "all_distances = []\n",
    "for i in range(len(x_data)):\n",
    "    all_distances.append(np.abs(np.mean(x_data[i]) - y_data[i][0]))\n",
    "all_distances = np.array(all_distances)\n",
    "\n",
    "print(\"sequence difference mean:\", np.mean(all_distances), \n",
    "      \"\\nsequence difference max: \", np.max(all_distances), \n",
    "      \"\\nsequence difference min: \", np.min(all_distances), \n",
    "      \"\\nsequence difference std: \", np.std(all_distances))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'collections.deque' object has no attribute 'head'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-87ca55923ef2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mclose_deque\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'collections.deque' object has no attribute 'head'"
     ]
    }
   ],
   "source": [
    "close_deque.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deque([53.439999, 53.619999, 53.529999])\n"
     ]
    }
   ],
   "source": [
    "print(close_deque)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deque([53.439999, 53.619999, 53.529999])\n"
     ]
    }
   ],
   "source": [
    "print(close_deque)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53.779999\n"
     ]
    }
   ],
   "source": [
    "print( target_close)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Again reset the graph\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Network parameters\n",
    "rnn_layer_sizes = [64, 32, 16]\n",
    "net_hidden_sizes = [8, 4]\n",
    "amount_epochs = 50\n",
    "learning_rate = 0.001\n",
    "batch_size = 32\n",
    "l2_strength = 0.001\n",
    "non_linearity = tf.nn.relu\n",
    "dropout_amount = 0.4\n",
    "max_grad_norm = 5.0\n",
    "decay = True\n",
    "cell_type = \"rnn\"\n",
    "\n",
    "# The input to the graph - the targets (close) and the inputs\n",
    "# (open). Also a placeholder to pass a variable dropout rate. \n",
    "net_input = tf.placeholder(tf.float32, shape=[None, sequence_length, 1])\n",
    "net_target = tf.placeholder(tf.float32, shape=[None, 1])\n",
    "dropout_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "# Create a stacked GRU with dropout using the rnn_layer_sizes\n",
    "# list to for the lstm memory size.\n",
    "cells = []\n",
    "for num_units in rnn_layer_sizes:\n",
    "    if cell_type == \"rnn\":\n",
    "        cell = tf.contrib.rnn.BasicRNNCell(num_units)\n",
    "    elif cell_type == \"gru\":\n",
    "        cell = tf.contrib.rnn.GRUCell(num_units)\n",
    "    elif cell_type == \"lstm\":\n",
    "        cell = tf.contrib.rnn.LSTMCell(num_units)\n",
    "    cell = tf.contrib.rnn.DropoutWrapper(cell=cell, \n",
    "                                         output_keep_prob=dropout_prob)\n",
    "    cells.append(cell)\n",
    "rnn_cell = tf.contrib.rnn.MultiRNNCell(cells)\n",
    "rnn_out, _ = tf.nn.dynamic_rnn(cell=rnn_cell,\n",
    "                               inputs=net_input,\n",
    "                               sequence_length=None,\n",
    "                               dtype=tf.float32)\n",
    "rnn_output = rnn_out[:, -1, :]\n",
    "                        \n",
    "# L2 regularisation to penalise the weights from growing too\n",
    "# large. Useful to prevent overfitting.\n",
    "regulariser = tf.contrib.layers.l2_regularizer(scale=l2_strength)\n",
    "\n",
    "# Build the network from the list of dimensions. Apply l2 and\n",
    "# dropout regularisation to the layers.\n",
    "net = rnn_output\n",
    "for size in net_hidden_sizes:\n",
    "    net = tf.layers.dense(inputs=net, \n",
    "                          units=size, \n",
    "                          activation=non_linearity, \n",
    "                          kernel_regularizer=regulariser)\n",
    "    net = tf.layers.dropout(inputs=net,\n",
    "                            rate=dropout_prob)\n",
    "    \n",
    "# The models prediction has a linear output. \n",
    "net_output = tf.layers.dense(inputs=net,\n",
    "                             units=1, \n",
    "                             activation=None, \n",
    "                             kernel_regularizer=regulariser)   \n",
    "\n",
    "# The main loss for penalising the network on how well it does.\n",
    "loss = tf.losses.mean_squared_error(labels=net_target, \n",
    "                                    predictions=net_output)\n",
    "\n",
    "# TensorFlows manner of applying l2 to the loss.\n",
    "l2_variables = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\n",
    "l2_loss = tf.contrib.layers.apply_regularization(regulariser, \n",
    "                                                 l2_variables)\n",
    "cost = loss + l2_loss\n",
    "\n",
    "# Train and initialisation TensorFlow operations to be ran\n",
    "# in the session.\n",
    "trainables = tf.trainable_variables()\n",
    "gradients = tf.gradients(cost, trainables)\n",
    "\n",
    "# Clip the gradients by a pre-defined max norm.\n",
    "clipped_gradients, _ = tf.clip_by_global_norm(gradients, max_grad_norm)\n",
    "\n",
    "# Add the clipped gradients to the optimizer.\n",
    "batch = tf.Variable(0)\n",
    "if decay:\n",
    "    decayed_learning_rate = tf.train.exponential_decay(learning_rate,\n",
    "                                                       batch * batch_size,\n",
    "                                                       117103,\n",
    "                                                       0.999,\n",
    "                                                       staircase=True)\n",
    "else:\n",
    "    decayed_learning_rate = learning_rate\n",
    "optimiser = tf.train.AdamOptimizer(decayed_learning_rate)\n",
    "train_op = optimiser.apply_gradients(zip(gradients, trainables), global_step=batch)\n",
    "\n",
    "init_op = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch: 0 \n",
      "mean:  721.33075 \n",
      "std:   2308.574 \n",
      "max:   35362.074 \n",
      "min:   1.7973319\n",
      "\n",
      "epoch: 1 \n",
      "mean:  2137.9998 \n",
      "std:   10196.638 \n",
      "max:   91839.15 \n",
      "min:   1.3896139\n",
      "\n",
      "epoch: 2 \n",
      "mean:  1135.5055 \n",
      "std:   3917.1047 \n",
      "max:   36914.82 \n",
      "min:   0.40987504\n",
      "\n",
      "epoch: 3 \n",
      "mean:  818.9724 \n",
      "std:   2705.2488 \n",
      "max:   20617.607 \n",
      "min:   1.3089634\n",
      "\n",
      "epoch: 4 \n",
      "mean:  771.45435 \n",
      "std:   2310.222 \n",
      "max:   16013.056 \n",
      "min:   1.2987884\n",
      "\n",
      "epoch: 5 \n",
      "mean:  1151.4763 \n",
      "std:   4266.7446 \n",
      "max:   41843.258 \n",
      "min:   1.401023\n",
      "\n",
      "epoch: 6 \n",
      "mean:  1033.2899 \n",
      "std:   3759.023 \n",
      "max:   41061.234 \n",
      "min:   0.89134514\n",
      "\n",
      "epoch: 7 \n",
      "mean:  1272.3566 \n",
      "std:   5045.561 \n",
      "max:   45063.926 \n",
      "min:   1.0281136\n",
      "\n",
      "epoch: 8 \n",
      "mean:  968.43945 \n",
      "std:   3626.5784 \n",
      "max:   30605.602 \n",
      "min:   1.3993125\n",
      "\n",
      "epoch: 9 \n",
      "mean:  1538.4425 \n",
      "std:   6435.4844 \n",
      "max:   82717.97 \n",
      "min:   1.4028656\n",
      "\n",
      "epoch: 10 \n",
      "mean:  940.7086 \n",
      "std:   3303.3738 \n",
      "max:   28783.959 \n",
      "min:   1.0855678\n",
      "\n",
      "epoch: 11 \n",
      "mean:  962.43976 \n",
      "std:   3597.654 \n",
      "max:   36250.53 \n",
      "min:   1.0256746\n",
      "\n",
      "epoch: 12 \n",
      "mean:  709.09216 \n",
      "std:   2103.6458 \n",
      "max:   18195.36 \n",
      "min:   1.7210103\n",
      "\n",
      "epoch: 13 \n",
      "mean:  877.76306 \n",
      "std:   2135.7693 \n",
      "max:   18337.723 \n",
      "min:   1.208025\n",
      "\n",
      "epoch: 14 \n",
      "mean:  743.5179 \n",
      "std:   2550.1023 \n",
      "max:   23522.775 \n",
      "min:   1.0257986\n",
      "\n",
      "epoch: 15 \n",
      "mean:  598.04456 \n",
      "std:   2280.8962 \n",
      "max:   27022.3 \n",
      "min:   1.0985842\n",
      "\n",
      "epoch: 16 \n",
      "mean:  502.12296 \n",
      "std:   1453.0592 \n",
      "max:   11277.217 \n",
      "min:   1.3294187\n",
      "\n",
      "epoch: 17 \n",
      "mean:  678.8283 \n",
      "std:   2266.054 \n",
      "max:   18978.008 \n",
      "min:   1.5220084\n",
      "\n",
      "epoch: 18 \n",
      "mean:  813.2718 \n",
      "std:   3044.144 \n",
      "max:   33037.957 \n",
      "min:   1.3961593\n",
      "\n",
      "epoch: 19 \n",
      "mean:  650.57446 \n",
      "std:   2533.8948 \n",
      "max:   30801.094 \n",
      "min:   1.9535822\n",
      "\n",
      "epoch: 20 \n",
      "mean:  461.71243 \n",
      "std:   1285.848 \n",
      "max:   9308.672 \n",
      "min:   1.4778377\n",
      "\n",
      "epoch: 21 \n",
      "mean:  765.4739 \n",
      "std:   3088.5786 \n",
      "max:   38398.785 \n",
      "min:   1.5717475\n",
      "\n",
      "epoch: 22 \n",
      "mean:  525.107 \n",
      "std:   2173.4873 \n",
      "max:   39354.53 \n",
      "min:   1.3959887\n",
      "\n",
      "epoch: 23 \n",
      "mean:  663.6563 \n",
      "std:   2810.2712 \n",
      "max:   37377.902 \n",
      "min:   1.8947695\n",
      "\n",
      "epoch: 24 \n",
      "mean:  565.4651 \n",
      "std:   1931.9443 \n",
      "max:   18971.273 \n",
      "min:   1.3078293\n",
      "\n",
      "epoch: 25 \n",
      "mean:  747.0266 \n",
      "std:   2407.769 \n",
      "max:   24035.062 \n",
      "min:   1.1504133\n",
      "\n",
      "epoch: 26 \n",
      "mean:  428.62766 \n",
      "std:   914.81476 \n",
      "max:   8609.99 \n",
      "min:   1.4985412\n",
      "\n",
      "epoch: 27 \n",
      "mean:  898.199 \n",
      "std:   4901.352 \n",
      "max:   60957.34 \n",
      "min:   1.2574794\n",
      "\n",
      "epoch: 28 \n",
      "mean:  460.12097 \n",
      "std:   1273.7709 \n",
      "max:   15834.922 \n",
      "min:   1.4924498\n",
      "\n",
      "epoch: 29 \n",
      "mean:  340.95383 \n",
      "std:   1656.8438 \n",
      "max:   30845.656 \n",
      "min:   1.3047452\n",
      "\n",
      "epoch: 30 \n",
      "mean:  308.372 \n",
      "std:   730.4364 \n",
      "max:   6376.96 \n",
      "min:   1.47466\n",
      "\n",
      "epoch: 31 \n",
      "mean:  278.42883 \n",
      "std:   752.4946 \n",
      "max:   9365.281 \n",
      "min:   1.1167052\n",
      "\n",
      "epoch: 32 \n",
      "mean:  479.72192 \n",
      "std:   1510.3036 \n",
      "max:   16250.943 \n",
      "min:   1.303555\n",
      "\n",
      "epoch: 33 \n",
      "mean:  533.24713 \n",
      "std:   2824.8994 \n",
      "max:   37941.336 \n",
      "min:   1.2087595\n",
      "\n",
      "epoch: 34 \n",
      "mean:  334.33847 \n",
      "std:   1012.7949 \n",
      "max:   15302.623 \n",
      "min:   1.1577783\n",
      "\n",
      "epoch: 35 \n",
      "mean:  281.9929 \n",
      "std:   502.22455 \n",
      "max:   4541.2334 \n",
      "min:   0.9578282\n",
      "\n",
      "epoch: 36 \n",
      "mean:  410.77472 \n",
      "std:   2234.317 \n",
      "max:   43879.633 \n",
      "min:   0.88173485\n",
      "\n",
      "epoch: 37 \n",
      "mean:  566.9886 \n",
      "std:   2355.495 \n",
      "max:   32327.535 \n",
      "min:   0.5871481\n",
      "\n",
      "epoch: 38 \n",
      "mean:  647.656 \n",
      "std:   2933.2778 \n",
      "max:   51932.668 \n",
      "min:   0.5826241\n",
      "\n",
      "epoch: 39 \n",
      "mean:  442.1644 \n",
      "std:   1657.715 \n",
      "max:   23122.742 \n",
      "min:   0.9606141\n",
      "\n",
      "epoch: 40 \n",
      "mean:  395.34644 \n",
      "std:   1141.5457 \n",
      "max:   12975.904 \n",
      "min:   0.5801489\n",
      "\n",
      "epoch: 41 \n",
      "mean:  456.6194 \n",
      "std:   2009.7504 \n",
      "max:   31091.387 \n",
      "min:   0.58930886\n",
      "\n",
      "epoch: 42 \n",
      "mean:  546.61206 \n",
      "std:   2319.2637 \n",
      "max:   41011.926 \n",
      "min:   0.60558605\n",
      "\n",
      "epoch: 43 \n",
      "mean:  450.87308 \n",
      "std:   1157.6057 \n",
      "max:   12497.347 \n",
      "min:   0.6062857\n",
      "\n",
      "epoch: 44 \n",
      "mean:  725.11365 \n",
      "std:   4221.784 \n",
      "max:   78031.17 \n",
      "min:   0.6160211\n",
      "\n",
      "epoch: 45 \n",
      "mean:  463.03293 \n",
      "std:   1357.4756 \n",
      "max:   14118.885 \n",
      "min:   1.1259723\n",
      "\n",
      "epoch: 46 \n",
      "mean:  644.5438 \n",
      "std:   2741.7087 \n",
      "max:   43199.53 \n",
      "min:   1.2991271\n",
      "\n",
      "epoch: 47 \n",
      "mean:  543.7264 \n",
      "std:   1531.8848 \n",
      "max:   25147.52 \n",
      "min:   1.3985949\n",
      "\n",
      "epoch: 48 \n",
      "mean:  468.24725 \n",
      "std:   1268.451 \n",
      "max:   11611.758 \n",
      "min:   0.8338425\n",
      "\n",
      "epoch: 49 \n",
      "mean:  486.58307 \n",
      "std:   1749.4109 \n",
      "max:   29752.09 \n",
      "min:   1.4595698\n",
      "\n",
      "Fold iteration:   0 \n",
      "Error mean:       486.58307 \n",
      "Error deviation:  1749.4109 \n",
      "\n",
      "\n",
      "epoch: 0 \n",
      "mean:  269.97693 \n",
      "std:   788.1715 \n",
      "max:   14382.365 \n",
      "min:   1.4954638\n",
      "\n",
      "epoch: 1 \n",
      "mean:  306.93976 \n",
      "std:   994.9277 \n",
      "max:   18773.14 \n",
      "min:   0.4925278\n",
      "\n",
      "epoch: 2 \n",
      "mean:  276.3662 \n",
      "std:   497.4219 \n",
      "max:   8241.568 \n",
      "min:   1.3013979\n",
      "\n",
      "epoch: 3 \n",
      "mean:  196.84534 \n",
      "std:   201.18399 \n",
      "max:   1586.4081 \n",
      "min:   1.4269798\n",
      "\n",
      "epoch: 4 \n",
      "mean:  205.1791 \n",
      "std:   343.79657 \n",
      "max:   8700.768 \n",
      "min:   0.83201957\n",
      "\n",
      "epoch: 5 \n",
      "mean:  244.73305 \n",
      "std:   339.6808 \n",
      "max:   7266.2236 \n",
      "min:   0.3981806\n",
      "\n",
      "epoch: 6 \n",
      "mean:  264.05383 \n",
      "std:   346.68982 \n",
      "max:   4112.739 \n",
      "min:   0.56492007\n",
      "\n",
      "epoch: 7 \n",
      "mean:  224.59602 \n",
      "std:   272.49094 \n",
      "max:   4564.1636 \n",
      "min:   0.8872452\n",
      "\n",
      "epoch: 8 \n",
      "mean:  224.22787 \n",
      "std:   218.72433 \n",
      "max:   2054.18 \n",
      "min:   0.93520164\n",
      "\n",
      "epoch: 9 \n",
      "mean:  274.3807 \n",
      "std:   288.83743 \n",
      "max:   4984.9434 \n",
      "min:   1.150168\n",
      "\n",
      "epoch: 10 \n",
      "mean:  174.0299 \n",
      "std:   245.56544 \n",
      "max:   4354.8955 \n",
      "min:   1.1016786\n",
      "\n",
      "epoch: 11 \n",
      "mean:  187.39748 \n",
      "std:   240.04623 \n",
      "max:   3550.024 \n",
      "min:   0.35658932\n",
      "\n",
      "epoch: 12 \n",
      "mean:  149.23027 \n",
      "std:   279.06827 \n",
      "max:   5030.4443 \n",
      "min:   0.85845196\n",
      "\n",
      "epoch: 13 \n",
      "mean:  184.61885 \n",
      "std:   215.88344 \n",
      "max:   2286.6433 \n",
      "min:   1.1671276\n",
      "\n",
      "epoch: 14 \n",
      "mean:  155.7645 \n",
      "std:   168.71356 \n",
      "max:   2243.525 \n",
      "min:   0.8131105\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    \n",
    "    amount_folds = 5\n",
    "    k_folds = KFold(n_splits=amount_folds)\n",
    "    fold_errors = []\n",
    "    fold_iteration = 0\n",
    "    \n",
    "    # Cross validate the dataset, using K-Fold.\n",
    "    for train_indices, test_indices in k_folds.split(x_data):\n",
    "\n",
    "        # Each new fold, reinitialise the network.\n",
    "        sess.run(init_op)\n",
    "        \n",
    "        # Training phase.\n",
    "        for epoch in range(amount_epochs):\n",
    "            \n",
    "            # Each new epoch, reshuffle the train set.\n",
    "            random_train_indices = np.random.permutation(train_indices)\n",
    "                \n",
    "            # Loop over the train set and optimise the network.\n",
    "            for begin in range(0, len(train_indices), batch_size):\n",
    "                end = begin + batch_size\n",
    "                batch_x = x_data[random_train_indices][begin:end]\n",
    "                batch_y = y_data[random_train_indices][begin:end]\n",
    "                \n",
    "                sess.run(train_op, feed_dict={\n",
    "                    net_input: batch_x,\n",
    "                    net_target: batch_y,\n",
    "                    dropout_prob: dropout_amount\n",
    "                })\n",
    "                \n",
    "            # Testing phase - record the error over the test set.\n",
    "            epoch_error = []\n",
    "            for begin in range(0, len(test_indices), batch_size):\n",
    "                end = begin + batch_size \n",
    "                batch_x = x_data[test_indices][begin:end]\n",
    "                batch_y = y_data[test_indices][begin:end]\n",
    "\n",
    "                error = sess.run(loss, feed_dict={\n",
    "                    net_input: batch_x,\n",
    "                    net_target: batch_y,\n",
    "                    dropout_prob: 1.0\n",
    "                }) \n",
    "                epoch_error.append(error)\n",
    "        \n",
    "            epoch_error = np.array(epoch_error).reshape((-1))    \n",
    "                \n",
    "            print('\\nepoch:', epoch,\n",
    "                  '\\nmean: ', np.mean(epoch_error), \n",
    "                  '\\nstd:  ', np.std(epoch_error),\n",
    "                  '\\nmax:  ', np.max(epoch_error),\n",
    "                  '\\nmin:  ', np.min(epoch_error))\n",
    " \n",
    "                \n",
    "        # Testing phase - record the error over the test set.\n",
    "        all_error = []\n",
    "        for begin in range(0, len(test_indices), batch_size):\n",
    "            end = begin + batch_size \n",
    "            batch_x = x_data[test_indices][begin:end]\n",
    "            batch_y = y_data[test_indices][begin:end]\n",
    "            \n",
    "            error = sess.run(loss, feed_dict={\n",
    "                net_input: batch_x,\n",
    "                net_target: batch_y,\n",
    "                dropout_prob: 1.0\n",
    "            }) \n",
    "            all_error.append(error)\n",
    "        \n",
    "        all_error = np.array(all_error).reshape((-1))\n",
    "        fold_errors.append(all_error)\n",
    "        \n",
    "        print(\"\\nFold iteration:  \", fold_iteration,\n",
    "              \"\\nError mean:      \", np.mean(all_error),\n",
    "              \"\\nError deviation: \", np.std(all_error),\n",
    "              \"\\n\")\n",
    "        fold_iteration += 1      \n",
    "        \n",
    "    fold_errors = np.array(fold_errors).reshape((amount_folds, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
